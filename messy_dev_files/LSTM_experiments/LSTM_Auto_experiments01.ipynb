{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I will be building out the architecture of a classification LSTM\n",
    "# At each time step, this LSTM will take in a vector representing autoencoder output\n",
    "# Its goal is to output whether or not the movie induces fear at each time step\n",
    "\n",
    "\n",
    "# First, import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting up the keras stuff\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import LSTM\n",
    "# my custom data_utils file\n",
    "from data_utils_local08 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 219, 12288)\n"
     ]
    }
   ],
   "source": [
    "# uploading the X values from the autoencoder\n",
    "X_input = np.load('./auto_output01.npy')\n",
    "print(X_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212,)\n"
     ]
    }
   ],
   "source": [
    "# uploading the Y values\n",
    "y_data_input = fear_oneHot(212, 'fear_annotations_part01/MEDIAEVAL18_7_Fear.txt')\n",
    "print(y_data_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 219, 12288)\n",
      "(3, 219, 12288)\n"
     ]
    }
   ],
   "source": [
    "# partition data into training and testing sets\n",
    "train_num = 4\n",
    "val_num = 3\n",
    "\n",
    "X_train_data = X_input[:train_num, :, :]\n",
    "X_valid_data = X_input[train_num:train_num+val_num, :, :]\n",
    "\n",
    "print(X_train_data.shape)\n",
    "print(X_valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# uploading the y training data\n",
    "timesteps = X_train_data.shape[1]\n",
    "data_dim = X_train_data.shape[2]\n",
    "\n",
    "# set up y_train_data master array\n",
    "Y_train_data = np.zeros([train_num, timesteps])\n",
    "\n",
    "# for each movie number between and including 7 and 13\n",
    "for num in range(train_num):\n",
    "    # create the appropriate path to the fear annotation data\n",
    "    #print(num)\n",
    "    path = os.path.join('fear_annotations_part01/MEDIAEVAL18_{}_Fear.txt'.format(7+num))\n",
    "    # create a one-hot vector\n",
    "    y_data = fear_oneHot(timesteps, path)\n",
    "    # add this one-hot vector to y_train_data\n",
    "    Y_train_data[num, :] = y_data\n",
    "print(Y_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# upload the Y validation set\n",
    "\n",
    "Y_valid_data = np.zeros([val_num, timesteps])\n",
    "\n",
    "# for each movie number in validation set\n",
    "for num in range(val_num):\n",
    "    # create the appropriate path to the fear annotation data\n",
    "    #print(num)\n",
    "    path = os.path.join('fear_annotations_part01/MEDIAEVAL18_{}_Fear.txt'.format(7+ train_num + num))\n",
    "    # create a one-hot vector\n",
    "    y_valid = fear_oneHot(timesteps, path)\n",
    "    # add this one-hot vector to y_train_data\n",
    "    Y_valid_data[num, :] = y_valid\n",
    "print(Y_valid_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constructing a many-to-many LSTM model in keras --> inspiration: https://stackoverflow.com/questions/43034960/many-to-one-and-many-to-many-lstm-examples-in-keras\n",
    "# training on autoencoder output\n",
    "model = Sequential()\n",
    "model.add(LSTM(timesteps, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))\n",
    "\n",
    "# flatten --> dense combo\n",
    "model.add(Flatten()) \n",
    "# add the final dense layer and then softmax\n",
    "model.add(Dense(timesteps, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compiling LSTM model\n",
    "# note that Ng used an Adam optimizer and categorical cross-entropy loss\n",
    "# but this is a binary classification problem so I think the parameters below should suffice\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples, validate on 3 samples\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.0413 - binary_accuracy: 0.8116 - val_loss: 1.2930 - val_binary_accuracy: 0.7763\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.9625 - binary_accuracy: 0.8116 - val_loss: 3.0324 - val_binary_accuracy: 0.7763\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.7071 - binary_accuracy: 0.8116 - val_loss: 3.1896 - val_binary_accuracy: 0.7747\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 9s 2s/step - loss: 2.0373 - binary_accuracy: 0.8139 - val_loss: 3.3674 - val_binary_accuracy: 0.7747\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 9s 2s/step - loss: 2.3690 - binary_accuracy: 0.8151 - val_loss: 3.3715 - val_binary_accuracy: 0.7763\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 9s 2s/step - loss: 2.4924 - binary_accuracy: 0.8128 - val_loss: 3.4875 - val_binary_accuracy: 0.7763\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 12s 3s/step - loss: 2.6560 - binary_accuracy: 0.8116 - val_loss: 3.5321 - val_binary_accuracy: 0.7717\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 12s 3s/step - loss: 2.8199 - binary_accuracy: 0.8128 - val_loss: 3.4882 - val_binary_accuracy: 0.7763\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 2.9049 - binary_accuracy: 0.8128 - val_loss: 3.6130 - val_binary_accuracy: 0.7747\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 12s 3s/step - loss: 3.0109 - binary_accuracy: 0.8116 - val_loss: 3.6303 - val_binary_accuracy: 0.7747\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 10s 3s/step - loss: 3.0721 - binary_accuracy: 0.8094 - val_loss: 3.6303 - val_binary_accuracy: 0.7747\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 10s 2s/step - loss: 3.0721 - binary_accuracy: 0.8094 - val_loss: 3.6303 - val_binary_accuracy: 0.7747\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 9s 2s/step - loss: 3.0721 - binary_accuracy: 0.8094 - val_loss: 3.6303 - val_binary_accuracy: 0.7747\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 10s 2s/step - loss: 3.0721 - binary_accuracy: 0.8094 - val_loss: 3.6303 - val_binary_accuracy: 0.7747\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 10s 2s/step - loss: 3.0721 - binary_accuracy: 0.8094 - val_loss: 3.6303 - val_binary_accuracy: 0.7747\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 10s 2s/step - loss: 3.0721 - binary_accuracy: 0.8094 - val_loss: 3.6303 - val_binary_accuracy: 0.7747\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 3.0721 - binary_accuracy: 0.8094 - val_loss: 3.6303 - val_binary_accuracy: 0.7747\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 3.0721 - binary_accuracy: 0.8094 - val_loss: 3.6303 - val_binary_accuracy: 0.7747\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 3.0721 - binary_accuracy: 0.8094 - val_loss: 3.6303 - val_binary_accuracy: 0.7747\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 13s 3s/step - loss: 3.0721 - binary_accuracy: 0.8094 - val_loss: 3.6303 - val_binary_accuracy: 0.7747\n",
      "finished training!\n"
     ]
    }
   ],
   "source": [
    "# running the LSTM model\n",
    "model.fit(X_train_data, Y_train_data, epochs = 20, validation_data=(X_valid_data, Y_valid_data))\n",
    "print(\"finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model prediction:\n",
      "[1.5196355e-07 3.9365766e-08 1.1666211e-08 2.9819137e-08 1.1119864e-07\n",
      " 2.5930452e-08 3.4395995e-08 9.4654418e-09 6.4271333e-08 9.0170644e-09\n",
      " 2.6179277e-08 2.1710793e-08 5.3078217e-08 8.9951104e-09 8.2388061e-09\n",
      " 1.5145423e-08 4.4607084e-08 2.6558396e-07 2.3572177e-07 2.8888974e-07\n",
      " 7.0404158e-06 3.7715604e-06 4.2565114e-07 4.3033810e-07 3.0542708e-07\n",
      " 8.1208827e-06 9.7397096e-06 1.8496506e-06 4.9017668e-05 3.4772820e-05\n",
      " 2.8764445e-05 2.4046751e-05 9.5762472e-05 1.7088316e-05 5.4705069e-05\n",
      " 3.5920857e-05 6.1904109e-07 8.6675769e-05 8.4074010e-05 1.1712875e-06\n",
      " 9.4848349e-07 7.0600770e-08 7.8952624e-08 5.6531437e-08 1.5574985e-07\n",
      " 4.7688481e-08 9.9840307e-08 4.6553765e-08 1.2508207e-07 5.0673677e-08\n",
      " 8.6592008e-06 6.0885242e-08 4.1828447e-05 1.0177346e-04 5.6338518e-05\n",
      " 1.7602665e-04 1.0976451e-04 6.3210391e-05 4.8141286e-05 7.8665085e-05\n",
      " 1.4424013e-07 1.2598485e-07 6.2352754e-08 2.9701752e-08 9.9999297e-01\n",
      " 9.9998963e-01 9.9998355e-01 9.9997711e-01 9.9999118e-01 9.9997938e-01\n",
      " 9.9991333e-01 9.9997795e-01 9.9989831e-01 9.9988902e-01 9.9999487e-01\n",
      " 9.9996805e-01 9.9998808e-01 9.9997830e-01 9.9993694e-01 1.0000000e+00\n",
      " 9.9993813e-01 9.9984384e-01 9.9994683e-01 9.9992716e-01 9.9992335e-01\n",
      " 9.9993169e-01 9.9998105e-01 9.9998403e-01 9.9992001e-01 9.9983692e-01\n",
      " 9.9975377e-01 9.9980181e-01 9.9983156e-01 9.9986124e-01 9.9981278e-01\n",
      " 9.9970907e-01 9.9998641e-01 9.9997973e-01 9.9998832e-01 9.9997520e-01\n",
      " 9.9988770e-01 9.9983025e-01 5.7965945e-06 1.4084890e-05 5.0357880e-06\n",
      " 9.9999166e-01 9.9992371e-01 9.9999416e-01 9.9984670e-01 9.9999452e-01\n",
      " 1.2112995e-06 1.6745758e-05 2.7436279e-06 2.2251775e-06 5.2086784e-06\n",
      " 2.7650816e-05 1.5956247e-06 6.9145171e-05 2.3589482e-05 2.1557023e-06\n",
      " 1.7281667e-07 3.8109807e-08 6.9038748e-08 6.5723619e-08 2.2163267e-06\n",
      " 2.4846993e-06 1.2998362e-05 2.1036107e-07 3.9719644e-08 9.9991512e-01\n",
      " 9.9978846e-01 9.9971670e-01 9.9995148e-01 9.9986863e-01 9.9998963e-01\n",
      " 9.9998438e-01 9.9991417e-01 9.9997067e-01 9.9982208e-01 9.9989545e-01\n",
      " 9.9981266e-01 9.9972826e-01 9.9988174e-01 9.9962187e-01 9.9981695e-01\n",
      " 9.9973053e-01 3.2802386e-06 1.8135951e-06 2.2585350e-07 1.5290090e-07\n",
      " 9.8716100e-08 5.5147684e-08 1.6084046e-07 8.9843297e-08 1.1041580e-07\n",
      " 1.8633457e-07 1.0573011e-07 3.2777038e-08 2.9529295e-08 5.9619538e-08\n",
      " 4.1390673e-09 1.5484844e-08 2.4012918e-08 5.5528761e-07 5.2561546e-07\n",
      " 1.1409039e-07 6.1923934e-06 1.8008652e-06 3.9657148e-06 4.6304444e-06\n",
      " 2.0465259e-07 2.3935394e-07 2.1794899e-07 2.1069729e-06 7.0275764e-06\n",
      " 1.0749046e-06 7.1672616e-06 1.8627512e-04 8.2364939e-05 3.4755747e-05\n",
      " 3.3382064e-04 3.1882202e-05 4.5523462e-05 4.4124008e-05 7.7684206e-05\n",
      " 2.4056431e-05 5.1917596e-05 4.4165688e-05 3.8925431e-05 8.5464431e-05\n",
      " 2.4471472e-05 2.4037581e-05 1.1475290e-04 3.3475089e-04 4.2419593e-05\n",
      " 3.9349903e-05 2.2074206e-04 7.5178505e-08 2.3397632e-08 5.1253306e-08\n",
      " 3.3345373e-08 2.7157158e-08 2.5550504e-08 2.8655132e-08 1.3753525e-08\n",
      " 7.3214622e-08 9.5419548e-09 5.6779108e-08 2.4171866e-08 2.1471735e-08\n",
      " 9.4868639e-08 6.8556119e-08]\n",
      "target:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "before 64:\n",
      "2.9701752e-08\n",
      "64:\n",
      "0.9999896\n",
      "rounded\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# trying to view the model output\n",
    "out = model.predict(X_train_data)\n",
    "print(\"model prediction:\")\n",
    "print(out[0])\n",
    "print(\"target:\")\n",
    "print(Y_train_data[0])\n",
    "\n",
    "print(\"before 64:\")\n",
    "print(out[0][63])\n",
    "print(\"64:\")\n",
    "print(out[0][65])\n",
    "\n",
    "print(\"rounded\")\n",
    "print(np.round(out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try visualizing this model at some point?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
